{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Practical Guide To A/B Tests in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://towardsdatascience.com/a-practical-guide-to-a-b-tests-in-python-66666f5c3b02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best practices that data scientists should follow pre-, during-, and after- experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A/B tests are effective and only rely on mild assumptions, and the most important assumption is the Stable Unit Treatment Value Assumption, SUTVA. It states that the treatment and control units don‚Äôt interact with each other; otherwise, the interference leads to biased estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A/B test can be roughly divided into three stages:\n",
    "- Stage 1 Pre-Test: run a power analysis to decide the sample size.\n",
    "- Stage 2 At-Test: keep an eye on the key metrics. Be aware of sudden drops.\n",
    "- Stage 3 Post-Test: analyze data and reach conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Scenario:\n",
    "TikTok develops a new animal filter and wants to assess its effects on users. They are interested in two key metrics:\n",
    "1. How does the filter affect user engagement (e.g., time spent on the app)?\n",
    "2. How does the filter affect user retention (e.g., active)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The company decides to hire a small group of very talented data scientists, and you are the team leader in charge of model selection and research design. After consulting with multiple stakeholders, you propose an A/B test and suggest the following best practices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 1 Pre-Test: Goal, Metrics, and Sample Size**\n",
    "- What is the goal of the test?\n",
    "- How to measure success?\n",
    "- How long should we run it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we want to clarify the goal of the test and relay it back to the team. As mentioned, the study aims to measure user engagement and retention after rolling out the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move to the metrics and decide how to measure the success. As a social networking app, we adopt the time spent on the app to measure user engagement and two boolean variables, metric 1 and metric 2 (described below), indicating if the user is active after 1 day and 7 days, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining question is: how long should we run the test? A common strategy is to stop the experiment once we observe a statistically significant result (e.g., a small p-value). Established data scientists strongly oppose p-hacking as it leads to biased results (Kohavi et al. 2020). On a related note, Airbnb has encountered the same problem when p-hacking leads to false positives (Experiments at Airbnb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instead, we should run a power analysis and decide a minimum sample size, according to three parameters:**\n",
    "- The significance Level, also denoted as alpha or Œ±: the probability of rejecting a null hypothesis when it is true. By rejecting a true null hypothesis, we falsely claim there is an effect when there is no actual effect. Thus, it is also called the probability of False Positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Statistical Power: the probability of correctly identifying the effect when there is indeed an effect. Power = 1 ‚Äî Type II Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Minimum Detectable Effect, MDE: to find a widely agreed upon MDE, our data team sits down with the PM and decides the smallest acceptable difference is 0.1. In other words, the difference between the two groups scaled by the standard deviation needs to be at least 0.1. Otherwise, the release won‚Äôt compensate for the business costs incurred (e.g., engineers‚Äô time, product lifecycle, etc.). For example, it won‚Äôt make any sense to roll out a new design if it only brings in a 0.000001% lift, even if it is statistically significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the bi-relationship between these three parameters and the required sample size:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Significance Level decreases ‚Üí Larger Sample Size\n",
    "- Statistical Power increases ‚Üí Larger Sample Size\n",
    "- The Minimum Detectable Effect decreases ‚Üí Larger Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we set the significance level at 5% (or alpha = 5%) and statistical power at 80%. Thus, the sample size is calculated by the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.webp\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "- œÉ¬≤: sample variance.\n",
    "- ùõø: the difference between the treatment and control groups (in percentage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the sample variance (œÉ¬≤), we typically run an A/A test that follows the same design thinking as an A/B test except assigning the same treatment to both groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the users into two groups and then assign the same treatment to both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 1571.000\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "# parameters for power analysis \n",
    "# effect_size has to be positive\n",
    "effect = 0.1\n",
    "alpha = 0.05\n",
    "power = 0.8\n",
    "# perform power analysis \n",
    "analysis = TTestIndPower()\n",
    "result = analysis.solve_power(effect, power = power,nobs1= None, ratio = 1.0, alpha = alpha)\n",
    "print('Sample Size: %.3f' % round(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need 1571 for each variant. In terms of how long we should run the test, it depends on how much traffic the app receives. Then, we divide the daily traffic equally into these two variants and wait until collecting a sufficiently large sample size (‚â•1571)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Practices**\n",
    "- Understand the goal of the experiment and how to measure the success.\n",
    "- Run an A/A test to estimate the variance of the metric. Check out my latest post on how to run and interpret A/A tests in Python.\n",
    "- Run a power analysis to obtain the minimum sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We roll out the test and initiate the data collection process. Here, we simulate the Data Generation Process (DGP) and artificially create variables that follow specific distributions. The true parameters are known to us, which comes in handy when comparing the estimated treatment effect to the true effects. In other words, we can evaluate the effectiveness of A/B tests and check to what extent they lead to unbiased results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five variables to be simulated in our case study:\n",
    "1. userid\n",
    "2. version\n",
    "3. minutes of plays\n",
    "4. user engagement after 1 day (metric_1)\n",
    "5. user engagement after 7 days (metric_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables 1 and 2: userid and version\n",
    "# We intentionally create 1600 control units and 1749 treated units to signal a potential Sample Ratio Mismatch, SRM.\n",
    "\n",
    "# variable 1: userid\n",
    "user_id_control = list(range(1,1601))# 1600 control\n",
    "user_id_treatment = list(range(1601,3350))# 1749 treated\n",
    "# variable 2: version \n",
    "import numpy as np\n",
    "control_status = [user_id_control]*1600\n",
    "treatment_status = [user_id_treatment]*1749"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable 3: minutes of plays\n",
    "# We simulate variable 3 (‚Äúminutes of plays‚Äù) as a normal distribution with a Œº of 30 minutes and œÉ¬≤ of 10. In specific, the mean for the control group is 30 minutes, and the variance is 10.\n",
    "# To recap, the effect parameter to the MDE is calculated as the difference between the two groups divided by the standard deviation (Œº_1 ‚Äî Œº_2)/œÉ_squared = 0.1. According to the formula, we obtain Œº_2 = 31. The variance is also 10.\n",
    "\n",
    "# for control group\n",
    "Œº_1 = 30\n",
    "œÉ_squared_1 = 10\n",
    "\n",
    "np.random.seed(123)\n",
    "minutes_control = np.random.normal(loc = Œº_1, scale = œÉ_squared_1, size = 1600)\n",
    "# for treatment group, which increases the user engagement by \n",
    "# according to the formula (Œº_1 ‚Äî Œº_2)/œÉ_squared = 0.1, we obtain Œº_2 = 31\n",
    "Œº_2 = 31\n",
    "œÉ_squared_2 = 10\n",
    "np.random.seed(123)\n",
    "minutes_treat = np.random.normal(loc = Œº_2, scale = œÉ_squared_2, size = 1749)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable 4: user engagement after 1 day, metric_1\n",
    "# Our simulation shows that the control group has 30% active (True) and 70% inactive (False) users after 1 day (metric_1), while the treatment has 35% active and 65% inactive users, respectively.\n",
    "Active_status = [True,False]\n",
    "# control \n",
    "day_1_control = np.random.choice(Active_status, 1600, p=[0.3,0.7])\n",
    "# treatment\n",
    "day_1_treatment = np.random.choice(Active_status, 1749, p=[0.35,0.65])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # variable 5: user engagement after 7 day, metric_2\n",
    "# The simulation data shows the control group has a 35% active user rate, while the treatment has a 25% after 7 days.\n",
    "\n",
    "# control \n",
    "day_7_control = np.random.choice(Active_status, 1600, p=[0.35,0.65])\n",
    "# treatment\n",
    "day_7_treatment = np.random.choice(Active_status, 1749, p=[0.25,0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true data contains a reversed pattern: the treatment performs better in the short term but the control group comes back and stands out after one week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs check if the A/B test picks up the reversed signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame({'userid':user_id_control+user_id_treatment,\n",
    "                            'version':control_status+treatment_status,\n",
    "                            'minutes_play':list(minutes_control)+list(minutes_treat),\n",
    "                            'day_1':list(day_1_control)+list(day_1_treatment),\n",
    "                            'day_7':list(day_7_control)+list(day_7_treatment)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>version</th>\n",
       "      <th>minutes_play</th>\n",
       "      <th>day_1</th>\n",
       "      <th>day_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>19.143694</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>39.973454</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>32.829785</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>14.937053</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>24.213997</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid                                            version  minutes_play  \\\n",
       "0       1  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...     19.143694   \n",
       "1       2  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...     39.973454   \n",
       "2       3  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...     32.829785   \n",
       "3       4  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...     14.937053   \n",
       "4       5  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...     24.213997   \n",
       "\n",
       "   day_1  day_7  \n",
       "0  False   True  \n",
       "1   True  False  \n",
       "2  False  False  \n",
       "3  False   True  \n",
       "4  False   True  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 3 After-Test: Data Analysis**\n",
    "After collecting enough data, we move to the last stage of experiments, which is data analysis. As a first step, it would be beneficial to check how many users fell into each variant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After taking a closer look, the control group has 29.7% active users, and the treatment has 35%.**\n",
    "\n",
    "- Naturally, we are interested in the following questions:\n",
    "\n",
    "- Is the higher retention rate in the treatment group statistically significant?\n",
    "\n",
    "- What is its variability?\n",
    "\n",
    "- If we repeat the process for 10,000 times, how often do we observe at least as extreme values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap can answer these questions. It is a resample strategy that repeatedly samples from the original data with replacements. According to the Central Limit Theorem, the distribution of the resample means approximately normally distributed (check my other posts on Bootstrap, in R or Python).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reversed pattern between 1-day and 7-day metrics supports the novelty effect as users become activated and intrigued by the new design, not because the change actually improves engagement. The novelty effect is popular in consumer-side A/B tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Practices**\n",
    "- SRM is a real concern. We apply a chi-square test to formally test for the SRM. If the p-value is smaller than the threshold (Œ± = 0.001), the randomization process does not work as expected.\n",
    "An SRM introduces selection bias that invalidates any test results.\n",
    "- Three fundamental statistical concepts to master: SRM, chi-square test, and bootstrap.\n",
    "- Compare short-term and long-term metrics to evaluate the novelty effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
